{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Approach-1"
      ],
      "metadata": {
        "id": "piIfeoIEnYaP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "9AkerCs0nUd0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/datasets/tesla.csv')\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "eCwjPvs1nUbD"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create features and labels\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ac9FaktcnUYb",
        "outputId": "6f4e9052-1b39-4304-9498-1d4f83c09be7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-6078241aa6e9>:2: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  data['Date'] = pd.to_datetime(data['Date'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lagged features\n",
        "for lag in range(1, 11):  # Increase the number of lagged features\n",
        "    data[f'Close_lag_{lag}'] = data['Close'].shift(lag)"
      ],
      "metadata": {
        "id": "4dF47AUlnUVs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add more technical indicators as features\n",
        "data['MA10'] = data['Close'].rolling(window=10).mean()\n",
        "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
        "data['RSI'] = data['Close'].diff().rolling(window=14).apply(lambda x: np.mean(np.where(x > 0, x, 0)) / (np.mean(np.abs(x)) + 1e-10), raw=True)\n",
        "data['EMA10'] = data['Close'].ewm(span=10, adjust=False).mean()\n",
        "data['EMA50'] = data['Close'].ewm(span=50, adjust=False).mean()\n",
        "\n",
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "2SLYCuwZnUSu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare feature and target arrays\n",
        "features = ['Open', 'High', 'Low', 'Adj Close', 'Volume', 'MA10', 'MA50', 'RSI', 'EMA10', 'EMA50'] + [f'Close_lag_{lag}' for lag in range(1, 11)]\n",
        "X = data[features]\n",
        "y = data['Close']"
      ],
      "metadata": {
        "id": "UaHGtoQJnUP5"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "_UQtrseZnUM8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "0RkuV7WWnUKR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
      ],
      "metadata": {
        "id": "TAhn5fm4n1Ol"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(256, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(LSTM(128, return_sequences=True))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.4))\n",
        "model.add(Dense(1))"
      ],
      "metadata": {
        "id": "319_1oAhn1LK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "SvPCsHJ-n1Io"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
        "history = model.fit(X_train_reshaped, y_train, epochs=500, batch_size=16, validation_split=0.2, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UcN_6IMn7q_",
        "outputId": "1bc465df-ed32-4ca4-bdc4-cd2f37af46cf"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "86/86 [==============================] - 11s 29ms/step - loss: 41984.4570 - val_loss: 46362.2461\n",
            "Epoch 2/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 39412.6055 - val_loss: 44535.4102\n",
            "Epoch 3/500\n",
            "86/86 [==============================] - 1s 16ms/step - loss: 37602.3281 - val_loss: 42515.0859\n",
            "Epoch 4/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 35935.9727 - val_loss: 41005.5039\n",
            "Epoch 5/500\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 34661.4766 - val_loss: 39591.8633\n",
            "Epoch 6/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 33432.0430 - val_loss: 38271.2227\n",
            "Epoch 7/500\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 32341.5547 - val_loss: 37006.5078\n",
            "Epoch 8/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 31229.3730 - val_loss: 35805.4023\n",
            "Epoch 9/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 30197.2246 - val_loss: 34658.0273\n",
            "Epoch 10/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 29322.4141 - val_loss: 33570.6016\n",
            "Epoch 11/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 28308.4551 - val_loss: 32505.4316\n",
            "Epoch 12/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 27376.4277 - val_loss: 31291.1289\n",
            "Epoch 13/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 26260.0996 - val_loss: 30206.8633\n",
            "Epoch 14/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 25372.6113 - val_loss: 29153.7383\n",
            "Epoch 15/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 24514.9102 - val_loss: 28124.4199\n",
            "Epoch 16/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 23457.8320 - val_loss: 27131.2598\n",
            "Epoch 17/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 22708.7168 - val_loss: 26172.4004\n",
            "Epoch 18/500\n",
            "86/86 [==============================] - 1s 17ms/step - loss: 21863.5195 - val_loss: 25232.2559\n",
            "Epoch 19/500\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 21034.1895 - val_loss: 24335.3301\n",
            "Epoch 20/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 20211.4570 - val_loss: 23452.7910\n",
            "Epoch 21/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 19547.7324 - val_loss: 22600.7363\n",
            "Epoch 22/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 18891.5488 - val_loss: 21775.4570\n",
            "Epoch 23/500\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 18116.3105 - val_loss: 20979.1758\n",
            "Epoch 24/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 17436.7812 - val_loss: 20195.9863\n",
            "Epoch 25/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 16822.7129 - val_loss: 19444.4004\n",
            "Epoch 26/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 16102.1182 - val_loss: 18721.7305\n",
            "Epoch 27/500\n",
            "86/86 [==============================] - 2s 18ms/step - loss: 15547.7314 - val_loss: 18011.1348\n",
            "Epoch 28/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 14952.8320 - val_loss: 17326.4512\n",
            "Epoch 29/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 14562.4814 - val_loss: 16667.3125\n",
            "Epoch 30/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 13799.0850 - val_loss: 16020.4639\n",
            "Epoch 31/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 13211.0146 - val_loss: 15396.6953\n",
            "Epoch 32/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 12839.2812 - val_loss: 14798.2549\n",
            "Epoch 33/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 12267.0010 - val_loss: 14214.6182\n",
            "Epoch 34/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 11797.1045 - val_loss: 13643.9346\n",
            "Epoch 35/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 11341.8867 - val_loss: 13101.6064\n",
            "Epoch 36/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 10865.4258 - val_loss: 12580.1133\n",
            "Epoch 37/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 10348.7148 - val_loss: 12073.8818\n",
            "Epoch 38/500\n",
            "86/86 [==============================] - 1s 16ms/step - loss: 9940.2910 - val_loss: 11576.2236\n",
            "Epoch 39/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 9655.5869 - val_loss: 11098.5225\n",
            "Epoch 40/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 9199.0703 - val_loss: 10643.0020\n",
            "Epoch 41/500\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 8814.3984 - val_loss: 10200.2646\n",
            "Epoch 42/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 8360.3672 - val_loss: 9771.0986\n",
            "Epoch 43/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 8037.4531 - val_loss: 9354.8457\n",
            "Epoch 44/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 7832.2305 - val_loss: 8959.2402\n",
            "Epoch 45/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 7475.6685 - val_loss: 8578.5371\n",
            "Epoch 46/500\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 7250.9214 - val_loss: 8207.6846\n",
            "Epoch 47/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 6769.1123 - val_loss: 7851.7246\n",
            "Epoch 48/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 6602.3433 - val_loss: 7508.9736\n",
            "Epoch 49/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 6259.0781 - val_loss: 7193.9502\n",
            "Epoch 50/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 5930.2993 - val_loss: 6875.0640\n",
            "Epoch 51/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 5831.2090 - val_loss: 6576.8784\n",
            "Epoch 52/500\n",
            "86/86 [==============================] - 1s 16ms/step - loss: 5476.8672 - val_loss: 6287.8076\n",
            "Epoch 53/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 5353.3125 - val_loss: 6010.3052\n",
            "Epoch 54/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 4976.1401 - val_loss: 5746.1235\n",
            "Epoch 55/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 4830.0322 - val_loss: 5497.7261\n",
            "Epoch 56/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 4631.4321 - val_loss: 5260.2988\n",
            "Epoch 57/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 4631.8828 - val_loss: 5021.0278\n",
            "Epoch 58/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 4222.5532 - val_loss: 4801.8887\n",
            "Epoch 59/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 4046.3987 - val_loss: 4593.4688\n",
            "Epoch 60/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 3938.1089 - val_loss: 4397.8936\n",
            "Epoch 61/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 3683.9246 - val_loss: 4206.0518\n",
            "Epoch 62/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 3521.4636 - val_loss: 4022.8811\n",
            "Epoch 63/500\n",
            "86/86 [==============================] - 2s 19ms/step - loss: 3370.9836 - val_loss: 3848.9668\n",
            "Epoch 64/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 3407.9902 - val_loss: 3682.5220\n",
            "Epoch 65/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 3179.8350 - val_loss: 3518.5283\n",
            "Epoch 66/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 3005.6213 - val_loss: 3365.2302\n",
            "Epoch 67/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 2919.0044 - val_loss: 3222.5686\n",
            "Epoch 68/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 2856.0117 - val_loss: 3075.8906\n",
            "Epoch 69/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 2809.6858 - val_loss: 2933.8977\n",
            "Epoch 70/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 2617.3291 - val_loss: 2802.9048\n",
            "Epoch 71/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 2522.4331 - val_loss: 2675.1167\n",
            "Epoch 72/500\n",
            "86/86 [==============================] - 2s 18ms/step - loss: 2373.5618 - val_loss: 2553.8345\n",
            "Epoch 73/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 2316.8030 - val_loss: 2441.7244\n",
            "Epoch 74/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 2328.1553 - val_loss: 2342.5120\n",
            "Epoch 75/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 2150.0869 - val_loss: 2219.6482\n",
            "Epoch 76/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 2150.1089 - val_loss: 2113.4648\n",
            "Epoch 77/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1924.4517 - val_loss: 2027.6053\n",
            "Epoch 78/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1907.1913 - val_loss: 1936.1194\n",
            "Epoch 79/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1770.0474 - val_loss: 1833.7307\n",
            "Epoch 80/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1745.2394 - val_loss: 1752.7397\n",
            "Epoch 81/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1775.2031 - val_loss: 1664.6882\n",
            "Epoch 82/500\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 1664.4205 - val_loss: 1592.0767\n",
            "Epoch 83/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 1646.3052 - val_loss: 1512.6210\n",
            "Epoch 84/500\n",
            "86/86 [==============================] - 2s 27ms/step - loss: 1495.5400 - val_loss: 1435.3583\n",
            "Epoch 85/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 1404.7329 - val_loss: 1367.4929\n",
            "Epoch 86/500\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 1279.6206 - val_loss: 1304.0543\n",
            "Epoch 87/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1342.6731 - val_loss: 1240.1733\n",
            "Epoch 88/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1367.5725 - val_loss: 1179.8579\n",
            "Epoch 89/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 1276.2703 - val_loss: 1115.7119\n",
            "Epoch 90/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1248.7627 - val_loss: 1086.9932\n",
            "Epoch 91/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1183.4856 - val_loss: 1007.5556\n",
            "Epoch 92/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 1098.1973 - val_loss: 951.1382\n",
            "Epoch 93/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 1119.5831 - val_loss: 898.2550\n",
            "Epoch 94/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 1117.4707 - val_loss: 851.9207\n",
            "Epoch 95/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 1036.3479 - val_loss: 802.1237\n",
            "Epoch 96/500\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 1042.4703 - val_loss: 762.0791\n",
            "Epoch 97/500\n",
            "86/86 [==============================] - 2s 18ms/step - loss: 1013.6844 - val_loss: 735.6138\n",
            "Epoch 98/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 946.3925 - val_loss: 676.4877\n",
            "Epoch 99/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 941.7119 - val_loss: 642.0082\n",
            "Epoch 100/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 932.6790 - val_loss: 615.9855\n",
            "Epoch 101/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 833.2446 - val_loss: 570.3760\n",
            "Epoch 102/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 854.2794 - val_loss: 582.0497\n",
            "Epoch 103/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 806.0399 - val_loss: 525.5532\n",
            "Epoch 104/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 859.2770 - val_loss: 479.3240\n",
            "Epoch 105/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 772.2402 - val_loss: 451.3664\n",
            "Epoch 106/500\n",
            "86/86 [==============================] - 2s 20ms/step - loss: 783.0407 - val_loss: 419.4297\n",
            "Epoch 107/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 715.6497 - val_loss: 396.2914\n",
            "Epoch 108/500\n",
            "86/86 [==============================] - 2s 20ms/step - loss: 722.6339 - val_loss: 375.9675\n",
            "Epoch 109/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 731.0525 - val_loss: 351.4130\n",
            "Epoch 110/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 679.7104 - val_loss: 374.4007\n",
            "Epoch 111/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 703.3802 - val_loss: 310.3351\n",
            "Epoch 112/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 665.0903 - val_loss: 309.9106\n",
            "Epoch 113/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 663.7984 - val_loss: 281.9796\n",
            "Epoch 114/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 654.9327 - val_loss: 261.3039\n",
            "Epoch 115/500\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 662.8197 - val_loss: 249.8819\n",
            "Epoch 116/500\n",
            "86/86 [==============================] - 1s 12ms/step - loss: 637.7501 - val_loss: 236.1684\n",
            "Epoch 117/500\n",
            "86/86 [==============================] - 1s 17ms/step - loss: 668.5746 - val_loss: 232.7232\n",
            "Epoch 118/500\n",
            "86/86 [==============================] - 2s 20ms/step - loss: 605.7456 - val_loss: 210.0637\n",
            "Epoch 119/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 652.8207 - val_loss: 199.6465\n",
            "Epoch 120/500\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 572.6170 - val_loss: 186.5466\n",
            "Epoch 121/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 540.0901 - val_loss: 176.1116\n",
            "Epoch 122/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 618.5784 - val_loss: 177.7156\n",
            "Epoch 123/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 581.2568 - val_loss: 181.1544\n",
            "Epoch 124/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 562.0416 - val_loss: 149.2230\n",
            "Epoch 125/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 545.8082 - val_loss: 153.3497\n",
            "Epoch 126/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 563.3999 - val_loss: 147.5356\n",
            "Epoch 127/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 586.9243 - val_loss: 157.7396\n",
            "Epoch 128/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 536.7214 - val_loss: 134.5772\n",
            "Epoch 129/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 560.1329 - val_loss: 115.8938\n",
            "Epoch 130/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 576.0945 - val_loss: 109.5105\n",
            "Epoch 131/500\n",
            "86/86 [==============================] - 1s 17ms/step - loss: 529.2197 - val_loss: 98.9358\n",
            "Epoch 132/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 563.6503 - val_loss: 100.3266\n",
            "Epoch 133/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 546.3585 - val_loss: 98.9362\n",
            "Epoch 134/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 546.4273 - val_loss: 100.6876\n",
            "Epoch 135/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 600.5284 - val_loss: 83.2493\n",
            "Epoch 136/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 562.5329 - val_loss: 76.2632\n",
            "Epoch 137/500\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 514.9825 - val_loss: 76.1382\n",
            "Epoch 138/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 484.1046 - val_loss: 77.7980\n",
            "Epoch 139/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 517.3752 - val_loss: 71.6514\n",
            "Epoch 140/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 500.4868 - val_loss: 65.9741\n",
            "Epoch 141/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 503.9360 - val_loss: 59.3378\n",
            "Epoch 142/500\n",
            "86/86 [==============================] - 2s 20ms/step - loss: 504.1992 - val_loss: 77.2623\n",
            "Epoch 143/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 539.5362 - val_loss: 65.7110\n",
            "Epoch 144/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 517.3773 - val_loss: 60.2205\n",
            "Epoch 145/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 538.1111 - val_loss: 50.4875\n",
            "Epoch 146/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 484.4361 - val_loss: 52.9868\n",
            "Epoch 147/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 507.3318 - val_loss: 84.1952\n",
            "Epoch 148/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 519.0328 - val_loss: 61.2538\n",
            "Epoch 149/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 553.1145 - val_loss: 52.6242\n",
            "Epoch 150/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 549.0553 - val_loss: 42.0591\n",
            "Epoch 151/500\n",
            "86/86 [==============================] - 2s 18ms/step - loss: 514.2741 - val_loss: 47.7227\n",
            "Epoch 152/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 489.7843 - val_loss: 46.5795\n",
            "Epoch 153/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 508.9636 - val_loss: 47.1764\n",
            "Epoch 154/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 489.9955 - val_loss: 132.3947\n",
            "Epoch 155/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 520.1548 - val_loss: 39.8938\n",
            "Epoch 156/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 502.1860 - val_loss: 36.5535\n",
            "Epoch 157/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 487.2665 - val_loss: 82.3832\n",
            "Epoch 158/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 473.3174 - val_loss: 39.4666\n",
            "Epoch 159/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 467.1278 - val_loss: 34.8131\n",
            "Epoch 160/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 475.9573 - val_loss: 37.7130\n",
            "Epoch 161/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 499.2921 - val_loss: 52.2113\n",
            "Epoch 162/500\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 495.0181 - val_loss: 41.3842\n",
            "Epoch 163/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 525.1426 - val_loss: 38.8950\n",
            "Epoch 164/500\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 534.8392 - val_loss: 53.4905\n",
            "Epoch 165/500\n",
            "86/86 [==============================] - 2s 19ms/step - loss: 522.6522 - val_loss: 32.6928\n",
            "Epoch 166/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 514.4396 - val_loss: 34.3376\n",
            "Epoch 167/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 479.3015 - val_loss: 40.3623\n",
            "Epoch 168/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 502.5753 - val_loss: 31.2231\n",
            "Epoch 169/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 501.9256 - val_loss: 29.3836\n",
            "Epoch 170/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 470.7771 - val_loss: 47.6904\n",
            "Epoch 171/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 524.8358 - val_loss: 29.8985\n",
            "Epoch 172/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 505.7124 - val_loss: 31.9013\n",
            "Epoch 173/500\n",
            "86/86 [==============================] - 1s 17ms/step - loss: 461.6150 - val_loss: 29.1990\n",
            "Epoch 174/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 511.0387 - val_loss: 31.9439\n",
            "Epoch 175/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 488.4559 - val_loss: 25.9330\n",
            "Epoch 176/500\n",
            "86/86 [==============================] - 1s 15ms/step - loss: 474.0239 - val_loss: 63.5251\n",
            "Epoch 177/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 486.3691 - val_loss: 25.4044\n",
            "Epoch 178/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 461.7914 - val_loss: 19.3902\n",
            "Epoch 179/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 492.2100 - val_loss: 23.6011\n",
            "Epoch 180/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 448.2174 - val_loss: 39.1133\n",
            "Epoch 181/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 455.6540 - val_loss: 27.0234\n",
            "Epoch 182/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 516.8958 - val_loss: 34.0644\n",
            "Epoch 183/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 478.6300 - val_loss: 26.7713\n",
            "Epoch 184/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 538.1317 - val_loss: 41.6881\n",
            "Epoch 185/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 489.9755 - val_loss: 50.4521\n",
            "Epoch 186/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 445.4704 - val_loss: 25.0416\n",
            "Epoch 187/500\n",
            "86/86 [==============================] - 2s 21ms/step - loss: 439.5016 - val_loss: 27.6337\n",
            "Epoch 188/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 465.5749 - val_loss: 42.1879\n",
            "Epoch 189/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 455.4386 - val_loss: 24.1331\n",
            "Epoch 190/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 451.5502 - val_loss: 34.8308\n",
            "Epoch 191/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 511.5087 - val_loss: 34.8326\n",
            "Epoch 192/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 484.5691 - val_loss: 40.7890\n",
            "Epoch 193/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 520.1445 - val_loss: 46.6087\n",
            "Epoch 194/500\n",
            "86/86 [==============================] - 1s 14ms/step - loss: 471.6662 - val_loss: 34.5011\n",
            "Epoch 195/500\n",
            "86/86 [==============================] - 1s 13ms/step - loss: 439.5879 - val_loss: 26.5552\n",
            "Epoch 196/500\n",
            "86/86 [==============================] - 1s 17ms/step - loss: 488.6966 - val_loss: 29.3839\n",
            "Epoch 197/500\n",
            "86/86 [==============================] - 2s 22ms/step - loss: 468.8741 - val_loss: 23.4663\n",
            "Epoch 198/500\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 425.8631 - val_loss: 27.5019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6fD5I4wn7nl",
        "outputId": "f311560e-06ea-4541-9891-6af026df47bb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 1s 5ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the predictions\n",
        "y_pred = y_pred.flatten()"
      ],
      "metadata": {
        "id": "AflbcZjhn7lH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "dG1Wb3-an7iO"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate a custom accuracy metric\n",
        "def custom_accuracy(y_true, y_pred, tolerance=0.05):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true) <= tolerance)\n",
        "\n",
        "accuracy = custom_accuracy(y_test, y_pred)"
      ],
      "metadata": {
        "id": "sDMdlImZoEFc"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R-squared (R2): {r2}')\n",
        "print(f'Custom Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-G3CToWnoGuk",
        "outputId": "4fadfd14-fed4-4541-db18-3edfb3f93b9f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 2.457856670133577\n",
            "Mean Squared Error (MSE): 21.226325370592562\n",
            "R-squared (R2): 0.9984027705763768\n",
            "Custom Accuracy: 87.65%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction on new data\n",
        "new_data = pd.DataFrame({\n",
        "    'Open': [300],\n",
        "    'High': [310],\n",
        "    'Low': [295],\n",
        "    'Adj Close': [305],\n",
        "    'Volume': [1000000],\n",
        "    'MA10': [305],\n",
        "    'MA50': [310],\n",
        "    'RSI': [0.01],\n",
        "    'EMA10': [305],\n",
        "    'EMA50': [310],\n",
        "    'Close_lag_1': [298],\n",
        "    'Close_lag_2': [299],\n",
        "    'Close_lag_3': [297],\n",
        "    'Close_lag_4': [296],\n",
        "    'Close_lag_5': [295],\n",
        "    'Close_lag_6': [294],\n",
        "    'Close_lag_7': [293],\n",
        "    'Close_lag_8': [292],\n",
        "    'Close_lag_9': [291],\n",
        "    'Close_lag_10': [290]\n",
        "})"
      ],
      "metadata": {
        "id": "e--QaTv0oJPn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_scaled = scaler.transform(new_data)\n",
        "new_data_reshaped = new_data_scaled.reshape((new_data_scaled.shape[0], 1, new_data_scaled.shape[1]))\n",
        "new_prediction = model.predict(new_data_reshaped)\n",
        "print(f'Predicted Close Price: {new_prediction[0][0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d-FKWhg4oMXH",
        "outputId": "b8e016c7-becd-4061-ca09-cbe417e800ad"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 25ms/step\n",
            "Predicted Close Price: 300.8011779785156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM Approach-2"
      ],
      "metadata": {
        "id": "Cts57RIyoOq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ],
      "metadata": {
        "id": "OztuRVCXoSz6"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "data = pd.read_csv('/content/drive/MyDrive/datasets/tesla.csv')\n",
        "data = data.dropna()"
      ],
      "metadata": {
        "id": "Qdurze28oSw2"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create features and labels\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "data.set_index('Date', inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtmrRUT5oSts",
        "outputId": "5bf10d35-0cde-4729-b504-2585d74b1621"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-6078241aa6e9>:2: UserWarning: Parsing dates in %d-%m-%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
            "  data['Date'] = pd.to_datetime(data['Date'])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create lagged features\n",
        "for lag in range(1, 11):  # Increase the number of lagged features\n",
        "    data[f'Close_lag_{lag}'] = data['Close'].shift(lag)"
      ],
      "metadata": {
        "id": "XD4Hd5v4oSrP"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add more technical indicators as features\n",
        "data['MA10'] = data['Close'].rolling(window=10).mean()\n",
        "data['MA50'] = data['Close'].rolling(window=50).mean()\n",
        "data['RSI'] = data['Close'].diff().rolling(window=14).apply(lambda x: np.mean(np.where(x > 0, x, 0)) / (np.mean(np.abs(x)) + 1e-10), raw=True)\n",
        "\n",
        "data.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "l3oo5dCNoSol"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare feature and target arrays\n",
        "features = ['Open', 'High', 'Low', 'Adj Close', 'Volume', 'MA10', 'MA50', 'RSI'] + [f'Close_lag_{lag}' for lag in range(1, 11)]\n",
        "X = data[features]\n",
        "y = data['Close']"
      ],
      "metadata": {
        "id": "8oQPygxOoSl4"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "GR7l8KfEoSjV"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "QSnmicUuoiAF"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape data for LSTM [samples, time steps, features]\n",
        "X_train_reshaped = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
        "X_test_reshaped = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))"
      ],
      "metadata": {
        "id": "W5xJePbBoh9B"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(128, return_sequences=True, input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(1))"
      ],
      "metadata": {
        "id": "l9AUdJYXoh6G"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')"
      ],
      "metadata": {
        "id": "dBlPUQiAoh34"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model with early stopping\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "history = model.fit(X_train_reshaped, y_train, epochs=300, batch_size=32, validation_split=0.2, callbacks=[early_stopping])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQQmN2wPoh1S",
        "outputId": "20081c25-1c01-4d32-f54b-d530c32d6f86"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "43/43 [==============================] - 7s 37ms/step - loss: 43540.7773 - val_loss: 48891.8438\n",
            "Epoch 2/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 41042.9844 - val_loss: 46263.5469\n",
            "Epoch 3/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 39598.2578 - val_loss: 45119.1328\n",
            "Epoch 4/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 38682.5234 - val_loss: 44126.2812\n",
            "Epoch 5/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 37780.7422 - val_loss: 43148.5547\n",
            "Epoch 6/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 36815.5273 - val_loss: 42086.6055\n",
            "Epoch 7/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 35866.4922 - val_loss: 41170.3750\n",
            "Epoch 8/300\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 35080.2695 - val_loss: 40390.0430\n",
            "Epoch 9/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 34409.3008 - val_loss: 39640.0859\n",
            "Epoch 10/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 33797.6055 - val_loss: 38932.0352\n",
            "Epoch 11/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 33153.1484 - val_loss: 38229.7188\n",
            "Epoch 12/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 32526.7168 - val_loss: 37565.8789\n",
            "Epoch 13/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 31974.0762 - val_loss: 36904.9961\n",
            "Epoch 14/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 31391.7168 - val_loss: 36279.0977\n",
            "Epoch 15/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 30836.1074 - val_loss: 35651.5938\n",
            "Epoch 16/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 30328.0430 - val_loss: 35036.0938\n",
            "Epoch 17/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 29722.7090 - val_loss: 34375.6719\n",
            "Epoch 18/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 29139.4160 - val_loss: 33754.3008\n",
            "Epoch 19/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 28663.2012 - val_loss: 33148.1953\n",
            "Epoch 20/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 28037.7676 - val_loss: 32542.7754\n",
            "Epoch 21/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 27549.7090 - val_loss: 31962.3887\n",
            "Epoch 22/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 27090.3496 - val_loss: 31383.1113\n",
            "Epoch 23/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 26470.6406 - val_loss: 30818.6328\n",
            "Epoch 24/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 26072.5508 - val_loss: 30260.5898\n",
            "Epoch 25/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 25588.6348 - val_loss: 29707.8887\n",
            "Epoch 26/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 25142.3301 - val_loss: 29174.5098\n",
            "Epoch 27/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 24579.3906 - val_loss: 28644.1836\n",
            "Epoch 28/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 24175.9277 - val_loss: 28122.8867\n",
            "Epoch 29/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 23766.9492 - val_loss: 27602.0820\n",
            "Epoch 30/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 23326.6816 - val_loss: 27098.1348\n",
            "Epoch 31/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 22834.7441 - val_loss: 26598.3711\n",
            "Epoch 32/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 22434.7520 - val_loss: 26112.1699\n",
            "Epoch 33/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 22042.2441 - val_loss: 25624.8477\n",
            "Epoch 34/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 21667.3203 - val_loss: 25155.1543\n",
            "Epoch 35/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 21201.5098 - val_loss: 24685.4688\n",
            "Epoch 36/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 20796.8555 - val_loss: 24224.8379\n",
            "Epoch 37/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 20402.4941 - val_loss: 23770.7012\n",
            "Epoch 38/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 19956.2969 - val_loss: 23322.5488\n",
            "Epoch 39/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 19524.6309 - val_loss: 22882.9316\n",
            "Epoch 40/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 19242.5820 - val_loss: 22448.0137\n",
            "Epoch 41/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 18853.5723 - val_loss: 22023.2539\n",
            "Epoch 42/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 18391.6094 - val_loss: 21607.7617\n",
            "Epoch 43/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 18066.7207 - val_loss: 21187.4277\n",
            "Epoch 44/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 17845.1426 - val_loss: 20778.0020\n",
            "Epoch 45/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 17405.2109 - val_loss: 20385.5371\n",
            "Epoch 46/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 17074.8945 - val_loss: 19991.0781\n",
            "Epoch 47/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 16738.4629 - val_loss: 19600.7031\n",
            "Epoch 48/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 16398.1191 - val_loss: 19223.0820\n",
            "Epoch 49/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 15983.8252 - val_loss: 18848.0762\n",
            "Epoch 50/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 15778.0615 - val_loss: 18476.3184\n",
            "Epoch 51/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 15426.4434 - val_loss: 18115.8457\n",
            "Epoch 52/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 15140.7861 - val_loss: 17755.8184\n",
            "Epoch 53/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 14816.1133 - val_loss: 17396.8477\n",
            "Epoch 54/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 14557.5625 - val_loss: 17056.0312\n",
            "Epoch 55/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 14278.3516 - val_loss: 16714.1758\n",
            "Epoch 56/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 13950.2256 - val_loss: 16375.9023\n",
            "Epoch 57/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 13627.3145 - val_loss: 16051.0615\n",
            "Epoch 58/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 13411.4961 - val_loss: 15723.0596\n",
            "Epoch 59/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 13060.3027 - val_loss: 15405.5713\n",
            "Epoch 60/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 12745.1426 - val_loss: 15090.7021\n",
            "Epoch 61/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 12586.3516 - val_loss: 14783.1191\n",
            "Epoch 62/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 12395.6250 - val_loss: 14479.6514\n",
            "Epoch 63/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 12085.5879 - val_loss: 14185.2041\n",
            "Epoch 64/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 11883.2393 - val_loss: 13885.1895\n",
            "Epoch 65/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 11553.0244 - val_loss: 13598.4355\n",
            "Epoch 66/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 11239.5244 - val_loss: 13313.9580\n",
            "Epoch 67/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 11130.9463 - val_loss: 13036.4648\n",
            "Epoch 68/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 10878.4268 - val_loss: 12759.8906\n",
            "Epoch 69/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 10647.5732 - val_loss: 12494.1885\n",
            "Epoch 70/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 10414.0840 - val_loss: 12229.0908\n",
            "Epoch 71/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 10064.4600 - val_loss: 11971.2705\n",
            "Epoch 72/300\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 9902.4121 - val_loss: 11711.4365\n",
            "Epoch 73/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 9634.3662 - val_loss: 11462.7793\n",
            "Epoch 74/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9528.4141 - val_loss: 11216.6152\n",
            "Epoch 75/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9387.3496 - val_loss: 10978.0693\n",
            "Epoch 76/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 9082.1074 - val_loss: 10739.0244\n",
            "Epoch 77/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 8931.5762 - val_loss: 10507.5732\n",
            "Epoch 78/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8808.1670 - val_loss: 10282.1826\n",
            "Epoch 79/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8509.4316 - val_loss: 10056.0361\n",
            "Epoch 80/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 8424.4619 - val_loss: 9833.9834\n",
            "Epoch 81/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 8004.4316 - val_loss: 9619.9785\n",
            "Epoch 82/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7945.1919 - val_loss: 9409.6104\n",
            "Epoch 83/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7792.2778 - val_loss: 9200.5615\n",
            "Epoch 84/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7573.7725 - val_loss: 8999.1631\n",
            "Epoch 85/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7487.9141 - val_loss: 8801.7285\n",
            "Epoch 86/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 7281.6416 - val_loss: 8602.1455\n",
            "Epoch 87/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7096.1553 - val_loss: 8410.6201\n",
            "Epoch 88/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 7054.3179 - val_loss: 8221.6113\n",
            "Epoch 89/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6790.1431 - val_loss: 8035.2559\n",
            "Epoch 90/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6620.7324 - val_loss: 7857.3286\n",
            "Epoch 91/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6417.3076 - val_loss: 7677.8701\n",
            "Epoch 92/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6449.3843 - val_loss: 7500.7974\n",
            "Epoch 93/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 6158.0552 - val_loss: 7332.1992\n",
            "Epoch 94/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 6000.0718 - val_loss: 7164.6602\n",
            "Epoch 95/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 5932.5303 - val_loss: 7002.2915\n",
            "Epoch 96/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 5841.6904 - val_loss: 6840.3486\n",
            "Epoch 97/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 5582.2349 - val_loss: 6685.2969\n",
            "Epoch 98/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 5476.0225 - val_loss: 6531.6514\n",
            "Epoch 99/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 5577.3779 - val_loss: 6381.9810\n",
            "Epoch 100/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 5343.5942 - val_loss: 6237.0654\n",
            "Epoch 101/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 5200.2383 - val_loss: 6090.9102\n",
            "Epoch 102/300\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 5097.1938 - val_loss: 5953.0991\n",
            "Epoch 103/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 4969.9937 - val_loss: 5812.6016\n",
            "Epoch 104/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 4853.0664 - val_loss: 5678.3721\n",
            "Epoch 105/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4744.9868 - val_loss: 5550.2378\n",
            "Epoch 106/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4541.1948 - val_loss: 5423.6235\n",
            "Epoch 107/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 4491.5479 - val_loss: 5299.7188\n",
            "Epoch 108/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4349.1860 - val_loss: 5175.1670\n",
            "Epoch 109/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4315.3301 - val_loss: 5056.4351\n",
            "Epoch 110/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4347.0942 - val_loss: 4941.0742\n",
            "Epoch 111/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 4091.5317 - val_loss: 4828.5137\n",
            "Epoch 112/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 4074.0237 - val_loss: 4716.3730\n",
            "Epoch 113/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3984.2271 - val_loss: 4608.8267\n",
            "Epoch 114/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 3920.7693 - val_loss: 4498.8784\n",
            "Epoch 115/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3792.4482 - val_loss: 4397.6445\n",
            "Epoch 116/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 3736.3811 - val_loss: 4293.5352\n",
            "Epoch 117/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3581.1858 - val_loss: 4197.1528\n",
            "Epoch 118/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 3630.2573 - val_loss: 4096.6748\n",
            "Epoch 119/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 3517.7734 - val_loss: 4003.4802\n",
            "Epoch 120/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 3354.3142 - val_loss: 3910.3838\n",
            "Epoch 121/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 3128.3994 - val_loss: 3817.1821\n",
            "Epoch 122/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 3242.3416 - val_loss: 3732.1614\n",
            "Epoch 123/300\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 3134.2231 - val_loss: 3643.0125\n",
            "Epoch 124/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 3038.1455 - val_loss: 3555.7271\n",
            "Epoch 125/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 2995.2498 - val_loss: 3474.9158\n",
            "Epoch 126/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 2941.5801 - val_loss: 3395.6821\n",
            "Epoch 127/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 2942.6592 - val_loss: 3317.6382\n",
            "Epoch 128/300\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 2947.4651 - val_loss: 3238.0798\n",
            "Epoch 129/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 2733.5051 - val_loss: 3162.1724\n",
            "Epoch 130/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 2669.4800 - val_loss: 3089.0664\n",
            "Epoch 131/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 2647.9438 - val_loss: 3013.6013\n",
            "Epoch 132/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 2577.1799 - val_loss: 2940.9021\n",
            "Epoch 133/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 2526.8159 - val_loss: 2872.0500\n",
            "Epoch 134/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 2594.8667 - val_loss: 2804.9404\n",
            "Epoch 135/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 2432.0496 - val_loss: 2736.0605\n",
            "Epoch 136/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 2375.8894 - val_loss: 2676.6968\n",
            "Epoch 137/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 2336.9285 - val_loss: 2607.0261\n",
            "Epoch 138/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 2348.5242 - val_loss: 2546.0667\n",
            "Epoch 139/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 2260.5635 - val_loss: 2482.3035\n",
            "Epoch 140/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 2197.5559 - val_loss: 2425.9121\n",
            "Epoch 141/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 2085.0518 - val_loss: 2365.5225\n",
            "Epoch 142/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 2099.0488 - val_loss: 2308.3882\n",
            "Epoch 143/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 2019.2522 - val_loss: 2255.9041\n",
            "Epoch 144/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1980.9985 - val_loss: 2199.1584\n",
            "Epoch 145/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 1998.5529 - val_loss: 2142.2493\n",
            "Epoch 146/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1904.2899 - val_loss: 2107.3918\n",
            "Epoch 147/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1887.8092 - val_loss: 2039.0687\n",
            "Epoch 148/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1867.5676 - val_loss: 1991.7316\n",
            "Epoch 149/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1801.0583 - val_loss: 1938.7969\n",
            "Epoch 150/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1797.9637 - val_loss: 1893.2397\n",
            "Epoch 151/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1684.9116 - val_loss: 1843.6536\n",
            "Epoch 152/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 1640.2704 - val_loss: 1804.3971\n",
            "Epoch 153/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1631.5217 - val_loss: 1752.8003\n",
            "Epoch 154/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1599.3457 - val_loss: 1711.5333\n",
            "Epoch 155/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1610.7814 - val_loss: 1667.8331\n",
            "Epoch 156/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1567.2252 - val_loss: 1637.3916\n",
            "Epoch 157/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1538.8274 - val_loss: 1585.0969\n",
            "Epoch 158/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1477.1486 - val_loss: 1548.3591\n",
            "Epoch 159/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1454.2166 - val_loss: 1500.9611\n",
            "Epoch 160/300\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 1393.0968 - val_loss: 1466.0889\n",
            "Epoch 161/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 1390.9309 - val_loss: 1425.4286\n",
            "Epoch 162/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 1334.3619 - val_loss: 1398.0896\n",
            "Epoch 163/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 1305.6427 - val_loss: 1353.7280\n",
            "Epoch 164/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 1337.1562 - val_loss: 1317.4779\n",
            "Epoch 165/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 1230.3256 - val_loss: 1285.1211\n",
            "Epoch 166/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 1269.1992 - val_loss: 1246.1414\n",
            "Epoch 167/300\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 1186.9802 - val_loss: 1210.7222\n",
            "Epoch 168/300\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 1195.9646 - val_loss: 1190.1598\n",
            "Epoch 169/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1179.3840 - val_loss: 1149.1304\n",
            "Epoch 170/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1148.1010 - val_loss: 1116.9052\n",
            "Epoch 171/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 1092.7836 - val_loss: 1084.7728\n",
            "Epoch 172/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 1104.4305 - val_loss: 1053.4320\n",
            "Epoch 173/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 1027.8540 - val_loss: 1025.3868\n",
            "Epoch 174/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 1029.3347 - val_loss: 994.3746\n",
            "Epoch 175/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 998.1433 - val_loss: 974.3731\n",
            "Epoch 176/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 984.7000 - val_loss: 943.0791\n",
            "Epoch 177/300\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 972.7443 - val_loss: 912.0433\n",
            "Epoch 178/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 963.6010 - val_loss: 896.7936\n",
            "Epoch 179/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 969.9667 - val_loss: 858.4622\n",
            "Epoch 180/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 943.1542 - val_loss: 831.1824\n",
            "Epoch 181/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 876.9532 - val_loss: 805.9268\n",
            "Epoch 182/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 902.4189 - val_loss: 789.1533\n",
            "Epoch 183/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 891.8802 - val_loss: 765.5977\n",
            "Epoch 184/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 878.4261 - val_loss: 735.7938\n",
            "Epoch 185/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 813.0781 - val_loss: 723.1534\n",
            "Epoch 186/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 761.1396 - val_loss: 687.3929\n",
            "Epoch 187/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 800.1263 - val_loss: 676.0591\n",
            "Epoch 188/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 785.8365 - val_loss: 647.6876\n",
            "Epoch 189/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 783.5712 - val_loss: 628.1245\n",
            "Epoch 190/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 719.4449 - val_loss: 609.1370\n",
            "Epoch 191/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 755.4742 - val_loss: 584.5878\n",
            "Epoch 192/300\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 769.7761 - val_loss: 565.8325\n",
            "Epoch 193/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 680.6442 - val_loss: 548.9979\n",
            "Epoch 194/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 673.9400 - val_loss: 532.1760\n",
            "Epoch 195/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 674.9191 - val_loss: 513.9045\n",
            "Epoch 196/300\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 683.4866 - val_loss: 496.3638\n",
            "Epoch 197/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 644.1915 - val_loss: 480.6229\n",
            "Epoch 198/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 635.6435 - val_loss: 467.7961\n",
            "Epoch 199/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 626.0162 - val_loss: 451.1624\n",
            "Epoch 200/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 629.0944 - val_loss: 438.2593\n",
            "Epoch 201/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 615.7201 - val_loss: 421.1547\n",
            "Epoch 202/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 575.3911 - val_loss: 407.0952\n",
            "Epoch 203/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 612.9476 - val_loss: 394.1771\n",
            "Epoch 204/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 582.8270 - val_loss: 385.2908\n",
            "Epoch 205/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 572.0975 - val_loss: 373.5062\n",
            "Epoch 206/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 560.1622 - val_loss: 357.1319\n",
            "Epoch 207/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 571.3076 - val_loss: 342.1870\n",
            "Epoch 208/300\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 542.7042 - val_loss: 337.4081\n",
            "Epoch 209/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 520.4396 - val_loss: 323.0581\n",
            "Epoch 210/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 515.9410 - val_loss: 325.8718\n",
            "Epoch 211/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 515.0702 - val_loss: 320.7129\n",
            "Epoch 212/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 543.0433 - val_loss: 326.5110\n",
            "Epoch 213/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 567.3220 - val_loss: 280.6471\n",
            "Epoch 214/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 476.3213 - val_loss: 273.6390\n",
            "Epoch 215/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 499.2821 - val_loss: 264.2097\n",
            "Epoch 216/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 482.4272 - val_loss: 253.8044\n",
            "Epoch 217/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 518.8068 - val_loss: 247.3898\n",
            "Epoch 218/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 470.8105 - val_loss: 242.4264\n",
            "Epoch 219/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 506.0697 - val_loss: 229.2412\n",
            "Epoch 220/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 464.9630 - val_loss: 219.5302\n",
            "Epoch 221/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 480.5750 - val_loss: 212.1371\n",
            "Epoch 222/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 488.8243 - val_loss: 206.9566\n",
            "Epoch 223/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 433.6857 - val_loss: 204.5916\n",
            "Epoch 224/300\n",
            "43/43 [==============================] - 1s 12ms/step - loss: 434.8294 - val_loss: 195.8546\n",
            "Epoch 225/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 475.5687 - val_loss: 183.5559\n",
            "Epoch 226/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 458.9087 - val_loss: 178.5867\n",
            "Epoch 227/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 422.4337 - val_loss: 180.5145\n",
            "Epoch 228/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 419.8600 - val_loss: 175.3211\n",
            "Epoch 229/300\n",
            "43/43 [==============================] - 1s 16ms/step - loss: 421.2068 - val_loss: 161.9555\n",
            "Epoch 230/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 468.6277 - val_loss: 155.6936\n",
            "Epoch 231/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 392.9939 - val_loss: 162.3300\n",
            "Epoch 232/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 400.5099 - val_loss: 146.1201\n",
            "Epoch 233/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 433.5115 - val_loss: 144.0909\n",
            "Epoch 234/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 399.0770 - val_loss: 147.8102\n",
            "Epoch 235/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 410.0020 - val_loss: 132.8625\n",
            "Epoch 236/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 381.2951 - val_loss: 130.1437\n",
            "Epoch 237/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 451.8169 - val_loss: 124.4480\n",
            "Epoch 238/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 398.5773 - val_loss: 120.1382\n",
            "Epoch 239/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 376.0052 - val_loss: 129.3569\n",
            "Epoch 240/300\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 347.1283 - val_loss: 114.7882\n",
            "Epoch 241/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 404.8133 - val_loss: 108.8048\n",
            "Epoch 242/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 392.6948 - val_loss: 102.6231\n",
            "Epoch 243/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 390.7284 - val_loss: 102.9005\n",
            "Epoch 244/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 361.0378 - val_loss: 97.3453\n",
            "Epoch 245/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 394.1640 - val_loss: 112.2755\n",
            "Epoch 246/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 376.0197 - val_loss: 95.2537\n",
            "Epoch 247/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 397.8524 - val_loss: 88.4194\n",
            "Epoch 248/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 350.5769 - val_loss: 93.4926\n",
            "Epoch 249/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 388.3956 - val_loss: 86.9871\n",
            "Epoch 250/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 396.9546 - val_loss: 91.0988\n",
            "Epoch 251/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 386.8735 - val_loss: 80.3094\n",
            "Epoch 252/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 344.5905 - val_loss: 81.5680\n",
            "Epoch 253/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 365.9502 - val_loss: 69.3177\n",
            "Epoch 254/300\n",
            "43/43 [==============================] - 0s 8ms/step - loss: 357.0798 - val_loss: 74.6192\n",
            "Epoch 255/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 345.4466 - val_loss: 81.3682\n",
            "Epoch 256/300\n",
            "43/43 [==============================] - 0s 12ms/step - loss: 366.5986 - val_loss: 75.1812\n",
            "Epoch 257/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 361.3086 - val_loss: 84.9260\n",
            "Epoch 258/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 359.9443 - val_loss: 71.2467\n",
            "Epoch 259/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 387.2906 - val_loss: 60.1391\n",
            "Epoch 260/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 325.3416 - val_loss: 64.0045\n",
            "Epoch 261/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 355.9007 - val_loss: 53.5516\n",
            "Epoch 262/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 338.5975 - val_loss: 52.6889\n",
            "Epoch 263/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 310.5709 - val_loss: 49.6733\n",
            "Epoch 264/300\n",
            "43/43 [==============================] - 0s 11ms/step - loss: 363.8999 - val_loss: 53.6401\n",
            "Epoch 265/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 359.8643 - val_loss: 67.2945\n",
            "Epoch 266/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 341.1421 - val_loss: 67.6867\n",
            "Epoch 267/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 337.3471 - val_loss: 47.2200\n",
            "Epoch 268/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 353.1282 - val_loss: 64.6348\n",
            "Epoch 269/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 342.6545 - val_loss: 42.3617\n",
            "Epoch 270/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 365.9605 - val_loss: 46.4887\n",
            "Epoch 271/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 355.7259 - val_loss: 60.9692\n",
            "Epoch 272/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 315.8277 - val_loss: 36.6408\n",
            "Epoch 273/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 315.7757 - val_loss: 46.2787\n",
            "Epoch 274/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 356.2394 - val_loss: 50.9936\n",
            "Epoch 275/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 389.2307 - val_loss: 39.8078\n",
            "Epoch 276/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 357.3625 - val_loss: 32.0261\n",
            "Epoch 277/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 339.9129 - val_loss: 48.6078\n",
            "Epoch 278/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 334.6654 - val_loss: 38.9270\n",
            "Epoch 279/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 325.6734 - val_loss: 35.6670\n",
            "Epoch 280/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 343.3195 - val_loss: 40.0560\n",
            "Epoch 281/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 356.1301 - val_loss: 30.8043\n",
            "Epoch 282/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 357.7246 - val_loss: 27.8684\n",
            "Epoch 283/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 309.5392 - val_loss: 27.4230\n",
            "Epoch 284/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 311.5438 - val_loss: 25.4833\n",
            "Epoch 285/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 328.0810 - val_loss: 24.9365\n",
            "Epoch 286/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 349.0325 - val_loss: 35.4694\n",
            "Epoch 287/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 327.4675 - val_loss: 28.2494\n",
            "Epoch 288/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 356.3875 - val_loss: 24.6576\n",
            "Epoch 289/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 322.7790 - val_loss: 22.8909\n",
            "Epoch 290/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 331.7158 - val_loss: 27.0145\n",
            "Epoch 291/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 338.6868 - val_loss: 25.6568\n",
            "Epoch 292/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 337.4002 - val_loss: 32.5524\n",
            "Epoch 293/300\n",
            "43/43 [==============================] - 1s 14ms/step - loss: 309.9500 - val_loss: 20.9476\n",
            "Epoch 294/300\n",
            "43/43 [==============================] - 1s 15ms/step - loss: 337.7210 - val_loss: 26.7706\n",
            "Epoch 295/300\n",
            "43/43 [==============================] - 1s 13ms/step - loss: 318.1278 - val_loss: 23.4446\n",
            "Epoch 296/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 303.4433 - val_loss: 25.0532\n",
            "Epoch 297/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 311.0198 - val_loss: 21.2081\n",
            "Epoch 298/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 323.6017 - val_loss: 18.5838\n",
            "Epoch 299/300\n",
            "43/43 [==============================] - 0s 10ms/step - loss: 341.1649 - val_loss: 34.3572\n",
            "Epoch 300/300\n",
            "43/43 [==============================] - 0s 9ms/step - loss: 319.1190 - val_loss: 67.5884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test_reshaped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5LFRQwcou_y",
        "outputId": "e7f252a9-8a09-4705-f9e2-e1ba786c7f3b"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 1s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flatten the predictions\n",
        "y_pred = y_pred.flatten()"
      ],
      "metadata": {
        "id": "27Za-VWWoxVX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "r2 = r2_score(y_test, y_pred)"
      ],
      "metadata": {
        "id": "YAYxcaNeoy0z"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate a custom accuracy metric\n",
        "def custom_accuracy(y_true, y_pred, tolerance=0.05):\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true) <= tolerance)\n",
        "\n",
        "accuracy = custom_accuracy(y_test, y_pred)"
      ],
      "metadata": {
        "id": "h8McOX_1o1F4"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Mean Absolute Error (MAE): {mae}')\n",
        "print(f'Mean Squared Error (MSE): {mse}')\n",
        "print(f'R-squared (R2): {r2}')\n",
        "print(f'Custom Accuracy: {accuracy * 100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hnetA0alo2my",
        "outputId": "623d2a73-bb49-40d8-cc7b-7d2c26560ab4"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE): 5.790148516981717\n",
            "Mean Squared Error (MSE): 63.5373168058954\n",
            "R-squared (R2): 0.9952189712477958\n",
            "Custom Accuracy: 89.74%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a prediction on new data\n",
        "new_data = pd.DataFrame({\n",
        "    'Open': [300],\n",
        "    'High': [310],\n",
        "    'Low': [295],\n",
        "    'Adj Close': [305],\n",
        "    'Volume': [1000000],\n",
        "    'MA10': [305],\n",
        "    'MA50': [310],\n",
        "    'RSI': [0.01],\n",
        "    'Close_lag_1': [298],\n",
        "    'Close_lag_2': [299],\n",
        "    'Close_lag_3': [297],\n",
        "    'Close_lag_4': [296],\n",
        "    'Close_lag_5': [295],\n",
        "    'Close_lag_6': [294],\n",
        "    'Close_lag_7': [293],\n",
        "    'Close_lag_8': [292],\n",
        "    'Close_lag_9': [291],\n",
        "    'Close_lag_10': [290]\n",
        "})"
      ],
      "metadata": {
        "id": "ymeuMZfUo49m"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_data_scaled = scaler.transform(new_data)\n",
        "new_data_reshaped = new_data_scaled.reshape((new_data_scaled.shape[0], 1, new_data_scaled.shape[1]))\n",
        "new_prediction = model.predict(new_data_reshaped)\n",
        "print(f'Predicted Close Price: {new_prediction[0][0]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rW4bOSiWo7M4",
        "outputId": "cfd88be8-20ce-48c2-84d5-9560ec6865c3"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 23ms/step\n",
            "Predicted Close Price: 286.53228759765625\n"
          ]
        }
      ]
    }
  ]
}